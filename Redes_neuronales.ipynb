{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "############################ LIMPIADOR DE MEMORIA ##############################\n",
        "################################################################################\n",
        "\n",
        "%reset -f  # Reinicia el entorno de trabajo para liberar memoria y evitar posibles conflictos\n",
        "\n",
        "\n",
        "################################################################################\n",
        "######################## INSTALACION DE PAQUETES NECESARIOS ####################\n",
        "################################################################################\n",
        "\n",
        "# Importamos todas las librerías necesarias para el análisis y modelado de datos\n",
        "\n",
        "import numpy as np  # Biblioteca para manejo de operaciones matemáticas\n",
        "import pandas as pd  # Biblioteca para manejo de datos en estructuras de tablas (DataFrames)\n",
        "import gdown  # Librería para descargar archivos desde Google Drive\n",
        "import matplotlib.pyplot as plt  # Librería para graficar\n",
        "from sklearn.model_selection import train_test_split  # Para dividir los datos en entrenamiento y validación\n",
        "from sklearn.metrics import mean_squared_error, r2_score  # Métricas estadísticas para evaluar el modelo\n",
        "from sklearn.preprocessing import MinMaxScaler  # Para normalizar los datos dentro de un rango especificado\n",
        "from scipy.stats import linregress  # Función para obtener la regresión lineal entre dos variables\n",
        "import tensorflow as tf  # Librería para implementar redes neuronales y modelos de machine learning\n",
        "from tensorflow.keras.models import Sequential  # Modelo secuencial de redes neuronales\n",
        "from tensorflow.keras.layers import Dense  # Capa completamente conectada en redes neuronales\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # Técnica para detener el entrenamiento si no hay mejora\n",
        "from tensorflow.keras import regularizers  # Herramientas para regularización en las redes neuronales\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################### CARGAMOS LOS DATOS #################################\n",
        "################################################################################\n",
        "\n",
        "# Especificamos los enlaces para descargar los archivos de Google Drive.\n",
        "# El ID del archivo es la parte del enlace de Google Drive que se encuentra entre 'd/' y '/view'\n",
        "\n",
        "file_id_validation = '1RlqzXzKuXF3majIFQ1ITqlPxxxl7xuch'  # ID del archivo de validación\n",
        "file_id_training = '1BjRir7yGs8vsU1D52n5YjB9HGfYv3ktT'  # ID del archivo de entrenamiento\n",
        "\n",
        "# Creamos las URLs de descarga directa para los archivos de Google Drive\n",
        "url_validation = f'https://drive.google.com/uc?id={file_id_validation}'\n",
        "url_training = f'https://drive.google.com/uc?id={file_id_training}'\n",
        "\n",
        "# Descargamos los archivos Excel desde Google Drive\n",
        "gdown.download(url_validation, 'validation_dataset.xlsx', quiet=False)  # Descargamos el archivo de validación\n",
        "gdown.download(url_training, 'dataset.xlsx', quiet=False)  # Descargamos el archivo de entrenamiento\n",
        "\n",
        "# Cargamos los datos desde los archivos Excel descargados\n",
        "df = pd.read_excel('dataset.xlsx', engine='openpyxl')  # Cargamos los datos de entrenamiento en un DataFrame\n",
        "dfv = pd.read_excel('validation_dataset.xlsx', engine='openpyxl')  # Cargamos los datos de validación\n",
        "\n",
        "# Verificamos que los datos se han cargado correctamente imprimiendo las primeras filas\n",
        "print(df.head())  # Imprimimos las primeras filas del conjunto de datos de entrenamiento\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# DATOS DE ENTRENAMIENTO\n",
        "# -----------------------------------------------------\n",
        "# Filtramos los datos para obtener solo las filas correspondientes al conjunto de entrenamiento\n",
        "df_training = df[df['ANNsubset'] == 'Training']  # Construimos un DataFrame solo con los datos de entrenamiento\n",
        " # (modificar a conveniencia o incluso suprimir en caso de no tener que distinguir)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# DATOS DE VALIDACIÓN\n",
        "# -----------------------------------------------------\n",
        "# Filtramos los datos para obtener solo las filas correspondientes al conjunto de validación\n",
        "df_validation = df[df['ANNsubset'] == 'Validation']  # Construimos un DataFrame solo con los datos de validación\n",
        " # (modificar a conveniencia o incluso suprimir en caso de no tener que distinguir)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###################################### STEC ####################################\n",
        "################################################################################\n",
        "\n",
        "# ------------------------------\n",
        "# 1. SELECCIÓN DE VARIABLES\n",
        "# ------------------------------\n",
        "# Para el conjunto de entrenamiento, seleccionamos las columnas de entrada y salida (modificar a conveniencia)\n",
        "X_train = df_training[['S(g/L)', 'T_cond(°C)', 'T_evap(°C)', 'F(L/h)']].values  # Variables de entrada\n",
        "y_train = df_training['STEC'].values  # Variable de salida (STEC)\n",
        "\n",
        "# Para el conjunto de validación, seleccionamos las columnas correspondientes\n",
        "X_val = df_validation[['S(g/L)', 'T_cond(°C)', 'T_evap(°C)', 'F(L/h)']].values  # Variables de entrada\n",
        "y_val = df_validation['STEC'].values  # Variable de salida (STEC)\n",
        "\n",
        "# También seleccionamos las mismas variables para otro conjunto de validación (en el archivo de validación 2)\n",
        "X_val2 = dfv[['S(g/L)', 'T_cond(°C)', 'T_evap(°C)', 'F(L/h)']].values  # Variables de entrada\n",
        "y_val2 = dfv['STEC'].values  # Variable de salida (STEC)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. NORMALIZACIÓN DE LOS DATOS\n",
        "# ------------------------------\n",
        "# Normalizamos los datos de entrada y salida para que estén en el rango [0.1, 0.9]\n",
        "scaler_X = MinMaxScaler(feature_range=(0.1, 0.9))  # Escalador para las variables de entrada\n",
        "scaler_y = MinMaxScaler(feature_range=(0.1, 0.9))  # Escalador para las variables de salida\n",
        "\n",
        "# Aplicamos la normalización a los datos de entrenamiento y validación\n",
        "X_train = scaler_X.fit_transform(X_train)  # Normalizamos las entradas de entrenamiento\n",
        "X_val = scaler_X.transform(X_val)  # Normalizamos las entradas de validación\n",
        "X_val2 = scaler_X.transform(X_val2)  # Normalizamos las entradas del segundo conjunto de validación\n",
        "\n",
        "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))  # Normalizamos las salidas de entrenamiento\n",
        "y_val = scaler_y.transform(y_val.reshape(-1, 1))  # Normalizamos las salidas de validación\n",
        "y_val2 = scaler_y.transform(y_val2.reshape(-1, 1))  # Normalizamos las salidas del segundo conjunto de validación\n",
        "\n",
        "# ------------------------------\n",
        "# 3. MODELO PRELIMINAR PARA DETECTAR RUIDO\n",
        "# ------------------------------\n",
        "# Creamos un modelo preliminar de red neuronal secuencial para predecir 'STEC'\n",
        "model_prelim = Sequential([\n",
        "    Dense(32, input_dim=4, activation='relu', kernel_initializer='glorot_normal'),  # Capa de entrada con 32 neuronas\n",
        "    Dense(16, activation='relu', kernel_initializer='glorot_normal'),  # Capa oculta con 16 neuronas\n",
        "    Dense(8, activation='relu', kernel_initializer='glorot_normal'),  # Capa oculta con 8 neuronas\n",
        "    Dense(1, activation='linear')  # Capa de salida (predicción de STEC)\n",
        "])\n",
        "# Dense(nº neuronas en la capa actual, numero de entradas, funcion de activación de la primera capa, criterio por el que se inicializan los pesos), Dense(...),...,nº de capas que quiera poner\n",
        "\n",
        "# Compilamos el modelo usando el optimizador 'adam' y la función de pérdida 'mse' (error cuadrático medio)\n",
        "model_prelim.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Entrenamos el modelo preliminar con los datos de entrenamiento\n",
        "model_prelim.fit(X_train, y_train, epochs=500, batch_size=8, verbose=0)\n",
        "# Epoch: numero de epocas de la fase de entrenamiento\n",
        "# batch_size: número de muestras que el modelo procesará en cada lote antes de actualizar los pesos.\n",
        "# verbose: controla el nivel de salida del entrenamiento, si es 0 no se muestra nada, si es 1 se muestra un progreso y si es 2 se muestra más información detallada\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 4. FILTRADO DE OUTLIERS (RUIDO)\n",
        "# ------------------------------\n",
        "# Realizamos predicciones sobre los datos de entrenamiento\n",
        "y_pred_init = model_prelim.predict(X_train)\n",
        "\n",
        "# Desnormalizamos las predicciones y los valores reales para obtener los resultados en su escala original\n",
        "y_true_init = scaler_y.inverse_transform(y_train)\n",
        "y_pred_inv_init = scaler_y.inverse_transform(y_pred_init)\n",
        "\n",
        "# Calculamos el error absoluto entre las predicciones y los valores reales\n",
        "errors = np.abs(y_true_init - y_pred_inv_init)\n",
        "\n",
        "# Definimos un umbral para considerar qué datos son \"outliers\" (valores atípicos)\n",
        "threshold = np.mean(errors) + 2 * np.std(errors)\n",
        "\n",
        "# Creamos una máscara para filtrar los datos que están por debajo del umbral de error\n",
        "mask = errors.flatten() < threshold\n",
        "\n",
        "# Aplicamos la máscara para eliminar los datos con ruido\n",
        "X_train_filtered = X_train[mask]  # Datos de entrada filtrados\n",
        "y_train_filtered = y_train[mask]  # Datos de salida filtrados\n",
        "\n",
        "# Imprimimos el número de datos filtrados\n",
        "print(f\"Filtrados {np.sum(~mask)} datos con ruido excesivo.\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. MODELO FINAL OPTIMIZADO\n",
        "# ------------------------------\n",
        "# Creamos el modelo final de red neuronal para predecir 'STEC'\n",
        "model = Sequential([\n",
        "    Dense(32, input_dim=4, activation='relu', kernel_initializer='glorot_normal'),  # Capa de entrada con 32 neuronas\n",
        "    Dense(16, activation='relu', kernel_initializer='glorot_normal'),  # Capa oculta con 16 neuronas\n",
        "    Dense(8, activation='relu', kernel_initializer='glorot_normal'),  # Capa oculta con 8 neuronas\n",
        "    Dense(1, activation='linear')  # Capa de salida (predicción de STEC)\n",
        "])\n",
        "\n",
        "# Compilamos el modelo con el optimizador 'adam' y la función de pérdida 'mse'\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "\n",
        "# Usamos EarlyStopping para evitar el sobreajuste (parar el entrenamiento si la validación no mejora)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "\n",
        "# Entrenamos el modelo con los datos filtrados y validación\n",
        "history = model.fit(X_train_filtered, y_train_filtered,\n",
        "                    epochs=1000,\n",
        "                    batch_size=8,\n",
        "                    verbose=0,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stop])\n",
        "\n",
        "# ------------------------------\n",
        "# 6. EVALUACIÓN DEL MODELO\n",
        "# ------------------------------\n",
        "def evaluate_and_plot(X, y, dataset_name):\n",
        "    # Realiza una predicción sobre los datos de entrada X usando el modelo entrenado\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Desnormaliza los valores reales (y) usando el scaler, para obtenerlos en su escala original\n",
        "    y_true = scaler_y.inverse_transform(y)\n",
        "\n",
        "    # Desnormaliza las predicciones del modelo (y_pred) para obtenerlas en su escala original\n",
        "    y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
        "\n",
        "    # Calcula el **Root Mean Squared Error (RMSE)** entre los valores reales y las predicciones\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred_inv))\n",
        "\n",
        "    # Calcula el **R²** (coeficiente de determinación) que indica cuán bien se ajusta el modelo a los datos\n",
        "    r2 = r2_score(y_true, y_pred_inv)\n",
        "\n",
        "    # Imprime los resultados del RMSE y R² con un formato de 3 decimales\n",
        "    print(f\"{dataset_name} - STEC - RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
        "\n",
        "    # Crea una figura para el gráfico con un tamaño específico\n",
        "    plt.figure(figsize=(6,5))\n",
        "\n",
        "    # Dibuja un gráfico de dispersión de los valores reales (y_true) frente a las predicciones (y_pred_inv)\n",
        "    plt.scatter(y_true, y_pred_inv, alpha=0.7)  # alpha controla la transparencia de los puntos\n",
        "\n",
        "    # Dibuja una línea roja discontinua que representa la relación perfecta (y_true == y_pred)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "\n",
        "    # Etiquetas para los ejes X e Y del gráfico\n",
        "    plt.xlabel('Real STEC')\n",
        "    plt.ylabel('Predicho STEC')\n",
        "\n",
        "    # Título del gráfico que incluye el nombre del dataset y el valor de R²\n",
        "    plt.title(f'{dataset_name}: STEC\\nR2={r2:.3f}, p={linregress(y_true.flatten(), y_pred_inv.flatten()).pvalue:.2e}')\n",
        "\n",
        "    # Ajusta el diseño para que los elementos del gráfico no se solapen\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Muestra el gráfico\n",
        "    plt.show()\n",
        "\n",
        "# Llamadas a la función para evaluar y graficar el rendimiento en diferentes datasets\n",
        "evaluate_and_plot(X_train_filtered, y_train_filtered, \"Entrenamiento\")  # Evaluación para el conjunto de entrenamiento filtrado\n",
        "evaluate_and_plot(X_val, y_val, \"Validación\")  # Evaluación para el conjunto de validación\n",
        "evaluate_and_plot(X_val2, y_val2, \"Validación Tabla 2\")  # Evaluación para otro conjunto de validación\n",
        "\n",
        "\n",
        "# Función para mostrar los pesos y sesgos aprendidos por el modelo\n",
        "def mostrar_pesos_y_sesgos(model):\n",
        "    # Configuración para mostrar todas las columnas en Google Colab\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "    # Iteramos sobre cada capa del modelo\n",
        "    for i, capa in enumerate(model.layers):\n",
        "        pesos, sesgos = capa.get_weights()\n",
        "\n",
        "        # Mostramos la matriz de pesos\n",
        "        df_pesos = pd.DataFrame(pesos)\n",
        "        print(f\"\\n{'='*10} Capa {i+1} - Matriz de Pesos (W^{i+1}) {'='*10}\")\n",
        "        print(df_pesos)\n",
        "\n",
        "        # Mostramos el vector de sesgo\n",
        "        df_sesgos = pd.DataFrame(sesgos.reshape(1, -1))\n",
        "        print(f\"\\n{'='*10} Capa {i+1} - Vector de Sesgo (b^{i+1}) {'='*10}\")\n",
        "        print(df_sesgos)\n",
        "\n",
        "# Llamada a la función (asegúrate de tener un modelo definido)\n",
        "mostrar_pesos_y_sesgos(model)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "###################################### PFLUX ###################################\n",
        "################################################################################\n",
        "\n",
        "# NOTA: los comentarios en esta celda son análogos a los de la predicción del STEC.\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Selección de variables\n",
        "# ------------------------------\n",
        "# entrenamiento\n",
        "X_train = df_training[['S(g/L)', 'T_cond(°C)', 'T_evap(°C)', 'F(L/h)']].values\n",
        "y_train = df_training['P_flux'].values\n",
        "\n",
        "# validacion\n",
        "X_val = df_validation[['S(g/L)', 'T_cond(°C)', 'T_evap(°C)', 'F(L/h)']].values\n",
        "y_val = df_validation['P_flux'].values\n",
        "\n",
        "# tabla 2 validacion\n",
        "X_val2 = dfv[['S(g/L)', 'T_cond(°C)', 'T_evap(°C)', 'F(L/h)']].values\n",
        "y_val2 = dfv['P_flux'].values\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Normalización [0.1, 0.9]\n",
        "# ------------------------------\n",
        "scaler_X = MinMaxScaler(feature_range=(0.1, 0.9))\n",
        "scaler_y = MinMaxScaler(feature_range=(0.1, 0.9))\n",
        "\n",
        "X_train = scaler_X.fit_transform(X_train)\n",
        "X_val = scaler_X.transform(X_val)\n",
        "X_val2 = scaler_X.transform(X_val2)\n",
        "\n",
        "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
        "y_val = scaler_y.transform(y_val.reshape(-1, 1))\n",
        "y_val2 = scaler_y.transform(y_val2.reshape(-1, 1))\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Entrenamiento preliminar para detectar ruido\n",
        "# ------------------------------\n",
        "model_prelim = Sequential([\n",
        "    Dense(32, input_dim=4, activation='relu', kernel_initializer='glorot_normal'),\n",
        "    Dense(16, activation='relu', kernel_initializer='glorot_normal'),\n",
        "    Dense(8, activation='relu', kernel_initializer='glorot_normal'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# Dense(nº neuronas en la capa actual, numero de entradas, funcion de activación de la primera capa, criterio por el que se inicializan los pesos), Dense(...),...,nº de capas que quiera poner\n",
        "\n",
        "model_prelim.compile(optimizer='adam', loss='mse')\n",
        "model_prelim.fit(X_train, y_train, epochs=500, batch_size=8, verbose=0)\n",
        "\n",
        "#\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Filtrado de outliers\n",
        "# ------------------------------\n",
        "y_pred_init = model_prelim.predict(X_train)\n",
        "y_true_init = scaler_y.inverse_transform(y_train)\n",
        "y_pred_inv_init = scaler_y.inverse_transform(y_pred_init)\n",
        "\n",
        "errors = np.abs(y_true_init - y_pred_inv_init)\n",
        "threshold = np.mean(errors) + 2 * np.std(errors)  # Umbral\n",
        "\n",
        "mask = errors.flatten() < threshold\n",
        "X_train_filtered = X_train[mask]\n",
        "y_train_filtered = y_train[mask]\n",
        "\n",
        "print(f\"Filtrados {np.sum(~mask)} datos con ruido excesivo.\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Modelo final optimizado\n",
        "# ------------------------------\n",
        "model = Sequential([\n",
        "    Dense(32, input_dim=4, activation='relu', kernel_initializer='glorot_normal'),\n",
        "    Dense(16, activation='relu', kernel_initializer='glorot_normal'),\n",
        "    Dense(8, activation='relu', kernel_initializer='glorot_normal'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train_filtered, y_train_filtered,\n",
        "                    epochs=1000,\n",
        "                    batch_size=8,\n",
        "                    verbose=0,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[early_stop])\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Evaluación\n",
        "# ------------------------------\n",
        "def evaluate_and_plot(X, y, dataset_name):\n",
        "    y_pred = model.predict(X)\n",
        "    y_true = scaler_y.inverse_transform(y)\n",
        "    y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred_inv))\n",
        "    r2 = r2_score(y_true, y_pred_inv)\n",
        "\n",
        "    print(f\"{dataset_name} - PFLUX - RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(y_true, y_pred_inv, alpha=0.7)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "    plt.xlabel('Real PFLUX')\n",
        "    plt.ylabel('Predicho PFLUX')\n",
        "    plt.title(f'{dataset_name}: PFLUX\\nR2={r2:.3f}, p={linregress(y_true.flatten(), y_pred_inv.flatten()).pvalue:.2e}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "evaluate_and_plot(X_train_filtered, y_train_filtered, \"Entrenamiento\")\n",
        "evaluate_and_plot(X_val, y_val, \"Validación\")\n",
        "evaluate_and_plot(X_val2, y_val2, \"Validación Tabla 2\")\n",
        "\n",
        "\n",
        "def mostrar_pesos_y_sesgos(model):\n",
        "    # Configuración para mostrar todas las columnas en Google Colab\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "    # Iteramos sobre cada capa del modelo\n",
        "    for i, capa in enumerate(model.layers):\n",
        "        pesos, sesgos = capa.get_weights()\n",
        "\n",
        "        # Mostramos la matriz de pesos\n",
        "        df_pesos = pd.DataFrame(pesos)\n",
        "        print(f\"\\n{'='*10} Capa {i+1} - Matriz de Pesos (W^{i+1}) {'='*10}\")\n",
        "        print(df_pesos)\n",
        "\n",
        "        # Mostramos el vector de sesgo\n",
        "        df_sesgos = pd.DataFrame(sesgos.reshape(1, -1))\n",
        "        print(f\"\\n{'='*10} Capa {i+1} - Vector de Sesgo (b^{i+1}) {'='*10}\")\n",
        "        print(df_sesgos)\n",
        "\n",
        "# Llamada a la función (asegúrate de tener un modelo definido)\n",
        "mostrar_pesos_y_sesgos(model)\n"
      ],
      "metadata": {
        "id": "yZVkpR68SiRw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}